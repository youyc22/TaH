# model config
dtype: bfloat16
mem_fraction_static: 0.90

# inference config
temperature: 0.6
top_p: 0.95
top_k: 20
min_p: 0.0
max_new_tokens: 4096
batch_size: 32        # total batch size
repeat_size: 1       # number of samples per question