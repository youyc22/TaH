### model ###
model:
  name: Qwen/Qwen3-0.6B-Base                # For TaH+ version
  # name: ./model/qwen3_0.6_base_pruned     # For TaH version
  torch_dtype: "bfloat16" 
  device_map: "auto"
  trust_remote_code: true
  attn_implementation: "sdpa"
  embedding_key: "model.embed_tokens"
  max_iter: 2
  iter_label_generator: "FixedIterLabelGenerator"
  iter_label_generator_kwargs: {}
  input_updater: "TrivialUpdater"          
  input_updater_kwargs:
    topk: 100
  iter_decider: "FixedLabelIterDecider"
  iter_decider_kwargs:
    label_type: "mismatch"
    max_iter: 2
  eval_iter_decider: "iter_decider"
  eval_iter_decider_kwargs:
    max_iter: 2
  output_updater: "AdditiveLogitsUpdater"
  output_updater_kwargs: {}
  adapter: "lora"
  adapter_kwargs: 
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: "all-linear"
    bias: "none"
  train_loss: "NextTokenPredLoss"
  eval_loss: "NextTokenPredLoss"

### data ###
data:
  train_data_path: data/processed_data/openr1_math/0_6/train
  eval_data_path: data/processed_data/openr1_math/0_6/eval
  output_dir: "output/openr1_math/0_6/"
  max_length: 8192
  iter_count_strategy: "mismatch"

### training ###
training:
  num_train_epochs: 5                     
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  gradient_checkpointing: false
  learning_rate: 4.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  max_grad_norm: 0.2
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs:
    min_lr_rate: 0.1
  logging_steps: 1
  save_strategy: "epoch"
  save_only_model: true
  save_total_limit: 50
  bf16: true
  # evaluation config
  eval_strategy: "steps"
  eval_steps: 40
  eval_on_start: true
  per_device_eval_batch_size: 1
  # wandb config
  report_to: "wandb"  # Options: "none", "wandb"
  wandb_project: "TaH"
  wandb_name: "openr1_0.6base_step1"
  # wandb_entity: ""