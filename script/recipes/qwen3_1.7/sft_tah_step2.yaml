### model ###
model:
  name: Qwen/Qwen3-1.7B-Base                # For TaH+ version
  # name: ./model/qwen3_1.7_base_pruned     # For TaH version
  tah_model_path: /path/to/step1-model/epoch4
  torch_dtype: "bfloat16" 
  device_map: "auto"
  trust_remote_code: true
  attn_implementation: "sdpa"
  embedding_key: "model.embed_tokens"
  max_iter: 2
  use_iter_embedding: false
  input_updater: "TrivialUpdater"
  input_updater_kwargs:
    topk: 100
  iter_label_generator: "FixedIterLabelGenerator"
  iter_label_generator_kwargs:
    max_iter: 2
  iter_decider: "MLPIterDecider"
  iter_decider_kwargs:
    topk: 100
    hidden_states_size: 2048
    hidden_states_layer_nums: [2,10,18,26]
    hidden_dims: [512, 512, 512, 512, 512, 512]
    expansion_factor: 4
    dropout_rate: 0.1
    normalize_input: false
    threshold: 0.8
  eval_iter_decider: "iter_decider"
  eval_iter_decider_kwargs: {}

  output_updater: "AdditiveLogitsUpdater" # Options: "NoneUpdater", "AdditiveLogitsUpdater"
  output_updater_kwargs: {}

  adapter: "lora" # Options: "lora", "cascade", "none", "multilora"
  adapter_kwargs:
    r: 32
    lora_alpha: 64
    lora_dropout: 0.1
    target_modules: "all-linear"
    base_grad: false
    adapter_grad: false
    bias: "none"
  train_loss: "IterDeciderLoss"
  train_loss_kwargs:
    pos_weight: 5.4
    skip_last_iter: true
    max_iter: 2
  eval_loss: "NextTokenPredLoss"
  eval_loss_kwargs: {}

### data ###
data:
  train_data_path: data/processed_data/openr1_math/1_7/train
  eval_data_path: data/processed_data/openr1_math/1_7/eval
  output_dir: "output/openr1_math/1_7/"
  max_length: 8192
  iter_count_strategy: "mismatch"

### training ###
training:
  freeze_component:
    - model.simple_base_model
  num_train_epochs: 2
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  gradient_checkpointing: false
  learning_rate: 3.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  max_grad_norm: 0.2
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs:
    min_lr_rate: 0.1
  logging_steps: 1
  save_strategy: "epoch"
  save_only_model: true
  save_total_limit: 10
  bf16: true
  # evaluation config
  eval_strategy: "steps"
  eval_steps: 40
  eval_on_start: true
  per_device_eval_batch_size: 1
  # wandb config
  report_to: "wandb"  # Options: "none", "wandb"
  wandb_project: "TaH"
  wandb_name: "openr1_1.7base_step2"
  # wandb_entity: ""