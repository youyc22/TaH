### Model Configuration ###
model:
  name: Qwen/Qwen3-0.6B-Base
  torch_dtype: "bfloat16" 
  device_map: "auto"
  trust_remote_code: true
  attn_implementation: "sdpa"
  embedding_key: "model.embed_tokens"
  max_iter: 1
  input_updater: "TrivialUpdater"
  input_updater_kwargs: {}
  iter_decider: "TrivialIterDecider"
  iter_decider_kwargs: {}
  output_updater: "NoneUpdater"
  output_updater_kwargs: {}
  adapter: "none"
  adapter_kwargs: {}
  train_loss: "NextTokenPredLoss"
  eval_loss: "NextTokenPredLoss"

### data ###
data:
  train_data_path: data/processed_data/openr1_math/0_6/train
  eval_data_path: data/processed_data/openr1_math/0_6/eval
  output_dir: "output/openr1_math/0_6/"
  max_length: 8192

### training ###
training:
  num_train_epochs: 5
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  gradient_checkpointing: false
  learning_rate: 4.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  max_grad_norm: 0.2
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs: 
    min_lr_rate: 0.1
  logging_steps: 1
  save_strategy: "epoch"
  save_only_model: true
  save_total_limit: 50
  bf16: true
  # evaluation
  eval_strategy: "steps"
  eval_steps: 40
  eval_on_start: true
  per_device_eval_batch_size: 1
  # wandb
  report_to: "wandb"  # Options: "none", "wandb"
  wandb_project: "TaH"
  wandb_name: "openr1_0.6base_standard"
  # wandb_entity: ""
